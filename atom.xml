<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>EscLife. Blog</title>
  
  <subtitle>炼丹、刷题--notes</subtitle>
  <link href="https://ywj1.github.io/atom.xml" rel="self"/>
  
  <link href="https://ywj1.github.io/"/>
  <updated>2021-06-07T10:42:30.979Z</updated>
  <id>https://ywj1.github.io/</id>
  
  <author>
    <name>EscLife</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Leetcode-21.5</title>
    <link href="https://ywj1.github.io/2021/06/07/Leetcode-21-5/"/>
    <id>https://ywj1.github.io/2021/06/07/Leetcode-21-5/</id>
    <published>2021-06-07T10:42:30.000Z</published>
    <updated>2021-06-07T10:42:30.979Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Summary of Transformer used in CV</title>
    <link href="https://ywj1.github.io/2021/06/01/Summary-of-Transformer-in-CV/"/>
    <id>https://ywj1.github.io/2021/06/01/Summary-of-Transformer-in-CV/</id>
    <published>2021-06-01T09:43:40.000Z</published>
    <updated>2021-06-09T07:57:44.247Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Summary-of-Transformer-Used-in-CV"><a href="#Summary-of-Transformer-Used-in-CV" class="headerlink" title="Summary of Transformer Used in CV"></a>Summary of Transformer Used in CV</h1><h2 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h2><h2 id="从图像分割上改进"><a href="#从图像分割上改进" class="headerlink" title="从图像分割上改进"></a>从图像分割上改进</h2><h3 id="DVT-Not-All-Images-are-Worth-16×16-Words-Dynamic-Vision-Transformers-with-Adaptive-Sequence-Length"><a href="#DVT-Not-All-Images-are-Worth-16×16-Words-Dynamic-Vision-Transformers-with-Adaptive-Sequence-Length" class="headerlink" title="DVT(Not All Images are Worth 16×16 Words: Dynamic Vision Transformers with Adaptive Sequence Length)"></a>DVT(Not All Images are Worth 16×16 Words: Dynamic Vision Transformers with Adaptive Sequence Length)</h3><h4 id="Abstract："><a href="#Abstract：" class="headerlink" title="Abstract："></a>Abstract：</h4><p>以ViT为代表的视觉Transformer通常将图片输入表征为固定数目的tokens。该文章认为每个图片有其自己的特点，其token numbers应该根据不同图片特征进行不同的划分。论文将图片从<code>easy-&gt;hard</code>进行划分，其中easy表示的切分的token number数目少，hard则为多。因此根据这个特点，论文提出了<strong>DVT(Dynamic Vision Transformer)</strong>来实现自适应的样本推理（即将样本切割成不同的token numbers）。并且针对模型由于Transformer级联导致的冗余计算，文章设计了<strong>feature reuse</strong>和<strong>relationship reuse</strong>。</p><h4 id="DVT"><a href="#DVT" class="headerlink" title="DVT"></a>DVT</h4><p>文章首先引出问题（很不错的写作思路），首先在T2T-ViT-12中使用不同的Tokens进行实验，发现若将token数目设置为4<em>4，准确率仅仅下降了15.9%，但是计算开销却下降了8.5倍。因此这一结果证明，正确识别占据大多数的较为“easy”的样本只需4\</em>4或者更少的token。</p><p><img src="/2021/06/01/Summary-of-Transformer-in-CV/DVT/T2T-ViT-12.png" alt="T2T-ViT-12"></p><p>因此受到上述现象的启发，论文提出了一个<strong>DVT(Dynamic Vision Transformer)</strong>如下图所示，针对每个样本选择一个合适数目token来进行表征：</p><p><img src="/2021/06/01/Summary-of-Transformer-in-CV/DVT/DVT.png" alt="DVT" style="zoom:75%;"></p><p>论文使用从小到大的token数目训练了一组Transformer模型，他们具有相同的基本结构，但是参数相互独立，以分别适应逐渐增多的token数目。对于任意测试样本，首先将其粗略表征为最小数目的token，输入第一个Transformer，以低计算成本迅速得到初步预测。而后判断预测结果是否可信，若可信，则直接输出当前结果并终止推理；若不可信，则将输入图片表征为更多的token，激活下一个Transformer进行更细粒度、但计算开销更大的的推理，得到结果之后再次判断是否可信，以此类推。论文采用将预测的置信度（confidence）与一个固定阈值进行比较的方式作为准出的判断准则。</p><p><strong>Feature reuse</strong></p><p>所有的Transformer都是为了提取分类特征，因此更多token数目的transformer被激活之后，一个更高效的做法是在前一步提取的特征中进一步提取而不是从0开始提取，因此论文提出了特征复用机制。将上面Transformer的最后一层提出，经过MLP和上采样，将其作为<strong>上下文嵌入(Context Embedding)</strong>，并且是以Concat方式加入。</p><p><img src="/2021/06/01/Summary-of-Transformer-in-CV/DVT/Feature-reuse.png" alt="Feature Reuse" style="zoom: 80%;"></p><p><strong>relationship reuse</strong></p><p>Transformer的self-attention模块能够整合图像的全局信息，能够有效的学习到长距离依赖特征。而关系复用指的是后面的Transformer可以使用先前的模型生成的自注意力图。将上游模型的全部注意力图以logits的形式进行整合，经MLP变换和上采样后，加入到下层每个注意力图的logits中。这样，下游模型每一层的attention模块都可灵活复用上游模型不同深度的全部attention信息，且这一复用信息的“强度”可以通过改变MLP的参数自动地调整。</p><p><img src="/2021/06/01/Summary-of-Transformer-in-CV/DVT/Relationship-reuse.png" alt="Relationship Reuse" style="zoom:80%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Summary-of-Transformer-Used-in-CV&quot;&gt;&lt;a href=&quot;#Summary-of-Transformer-Used-in-CV&quot; class=&quot;headerlink&quot; title=&quot;Summary of Transformer Use</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Transformer" scheme="https://ywj1.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer In Object Detection</title>
    <link href="https://ywj1.github.io/2021/05/27/Transformer-In-Object-Detection/"/>
    <id>https://ywj1.github.io/2021/05/27/Transformer-In-Object-Detection/</id>
    <published>2021-05-27T06:15:04.000Z</published>
    <updated>2021-06-08T07:45:50.847Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer-in-Object-Detection"><a href="#Transformer-in-Object-Detection" class="headerlink" title="Transformer in Object Detection"></a>Transformer in Object Detection</h1><p>Transformer在NLP中应用很广泛，但是在CV中确实十分少见，究其原因，主要是因为CV中使用的是图像，都是2维的数据，而NLP中是以1维数据为主。因此在ViT之前，CV领域主要是将其思想，self-Attention借用在CNN网络中，即提出了通道注意力等各种各样的注意力代表性网络是：<a href="https://arxiv.org/abs/1709.01507">SeNet</a>等。随着ViT的出现，Transformer在CV领域如喷泉搬涌出，Transformer在CV中的应用越来越多。这次主要介绍Transformer在目标检测（Object Detection）中的应用。主要分为三个部分，首先介绍最基础的<a href="&quot;Transformer&quot;">Transfomer</a>，随后介绍<a href="&quot;DETR&quot;">DETR</a>，最后在DETR中改进的<a href="&quot;Deformable DETR&quot;">Deformable DETR</a>。</p><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>Transformer结构首先在<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a>中提出，文中主要提出了Transformer模型，构建里序列模型的全局关系，不像RNN一样，需要一个个进行预测，而是可以实现并行化运行。对于直接从事或者学习CV方向的来说，首先我们了解一下什么叫做Embedding（有过NLP方向可以略过此部分）。</p><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p><strong>数学角度</strong>：</p><p>​    Embedding实际上是一个映射（mapping），<code>f:x-&gt;y</code>，其中f是一个函数，实现x到y的映射，并且f具有俩个特征：injective（单射性：不同的x映射到不同的y）2.structure-preserving（结构保存，或者说特征保存：如果<code>x1&gt;x2</code>通过f得到的y1,y2满足<code>y1&gt;y2</code>）。因此从数学角度来说，Embedding即找到一个函数，实现<code>x-&gt;y</code>的映射，并且这个函数具有如上所述俩个特征。</p><p><strong>神经网络</strong>：</p><p>​    将离散变量—&gt;连续的变量，减少离散变量的维数，并且在转换之后也可以有意义的表示该变量。Embedding主要有三个目的：</p><ol><li>在embedding空间中查找最领近，在用户推荐系统中这可以根据用户的兴趣进行推荐（后续通过与One-hot进行对比可以理解的更为清楚）</li><li>最为监督性学习任务的输入</li><li>用于可视化不同离散变量之间的关系（这里有一个很好玩的网络，可以用于可视化Embedding，是Tensorflow开发的<a href="https://projector.tensorflow.org/">projector</a>，是一个在线应用程序）</li></ol><p><strong>One-Hot Encoding与Embedding之间的对比</strong>：<br>例如在NLP学习中，有如下的几组词，首先通过One-hot encoding：</p><pre class="line-numbers language-lang-C++"><code class="language-lang-C++">words = ['I', 'have', 'an', 'apple']One_hot_words = [[1, 0, 0, 0],                [0, 1, 0, 0],                [0, 0, 1, 0],                [0, 0, 0, 1]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从上述的One-hot encoding可以看出每个变量是没有关系的，并且随着words的数量越来越多，One-hot向量的维数也会越来越大，这显然不是我们所希望的。</p><p>因此提出了Embedding，通过压缩向量的维数，使用Embedding neural network和supervised learning（即简易的神经网络）来进行学习，来找到最适合的表述以及挖掘内在联系。如下面的例子（PS：随便举的）</p><pre class="line-numbers language-lang-python"><code class="language-lang-python">words = ['I', 'have', 'an', 'apple']Embedding = [[0.84, 0.48],            [0.32, 0.57],            [0.23, -0.43],            [-0.87, -0.94]]//通过计算向量的内积，可以得出不同单词之间的相关度。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>引用：</p><ul><li><a href="https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526">https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526</a></li><li><a href="https://zhuanlan.zhihu.com/p/46016518">https://zhuanlan.zhihu.com/p/46016518</a></li></ul><h3 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need"></a>Attention Is All You Need</h3><h4 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h4><p>如下图可见Transformer的整体架构：</p><p><img src="/2021/05/27/Transformer-In-Object-Detection/Transformer.png" alt="Transformer" style="zoom:80%;"></p><p>模型主要由左边的Encoder和右边的Decoder组成，最终输出预测的序列模型的概率。</p><p>对于早期，处理序列模型，最先想到的是RNN，他的输入是一串Vector Sequence，输出也是另外一串Vector Sequence，但是对于RNN来说是很不好并行化的，因为需要通过前一个字符来预测下一个字符，这个过程是很难并行化的。因此这篇文章提出了self-Attention，使用self-Attention来取代RNN所做的事情。（self-Attention部分会在Encoder部分重点讲述）</p><p><img src="/2021/05/27/Transformer-In-Object-Detection/RNN.png" alt="RNN" style="zoom:80%;"></p><p>接下来根据上述模型顺序进行讲解：（后面会补上代码讲述！）</p><ul><li>Input Embedding</li><li>Encoder</li><li>Decoder</li></ul><h4 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h4><p><img src="/2021/05/27/Transformer-In-Object-Detection/Input-Embedding.png" alt="输入结构"></p><p>由图中可以看到<code>The Sequence input to Encoder = Input Embedding + Positional Encoding</code>，关于Embedding在上面的Embedding中已经讲述了，接下来主要讲解Positional Encoding。</p><h5 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h5><p><strong>为什么要使用Positional Encoding？</strong></p><p>由于在RNN中，序列处理是有顺序的，因此不需要通过位置编码来确定每个位置的信息，但是在Transformer中，由于使用的并行化，因此一个单词向量对于不同位置的单词向量的效果是相同的（如：在序列”我打了你”中，由于没有位置信息，其中并不能明确主宾关系，无法准确翻译我和你，<font color="red">这只是我的理解，毕竟我对于NLP不太懂</font>，总而言之，就是无法确定不同距离单词向量之间的关系，因为self-Attention关系，每个单词之间效果是相同的）</p><p><strong>Positional Encoding</strong></p><p><em>固定的位置编码（Absolute positional Encoding)</em>：</p><script type="math/tex; mode=display">\left\{\begin{aligned}PE(pos, 2i) = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\PE(pos, 2i+1) = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\\end{aligned}\right.</script><p>其中i的取值范围是$[0,…,d_{model}/2)$，设计这个公式的优点在于：</p><ul><li>每个位置都有唯一的Positional encoding</li><li>PE能够适应比训练集中更长的数据</li><li>对于不同单词向量的相对位置更好计算</li></ul><p><em>不固定的位置编码（Relative Positional Encoding）</em>：</p><p>即通过训练学习得到Positional Encoding。</p><p>在Transformer这篇文章中，这俩种方法没有很大的区别，因此选用了不固定的Positional Encoding。但是在目标检测中，俩种方法相差还是有的(<a href="https://arxiv.org/pdf/2104.13840.pdf">Twins-SVT</a>)。</p><p><strong>为什么使用element-wise summation 而不是concatenate？</strong></p><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p><img src="/2021/05/27/Transformer-In-Object-Detection/Encoder.png" alt="Encoder"></p><p>如图所示Transformer结构中，主要包括三个部分：</p><ul><li>Multi-Head Attention</li><li>Add&amp;Norm层</li><li>Feed Forward层</li></ul><h5 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h5><p>在介绍Multi-Head Attention之前，首先我们应该知道什么叫做Head Attention。可以看到图中有Q,K,V三个向量，这三个向量是分别通过$XW^q,XW^k,XW^v$来得到的三个向量</p><ul><li>Q：query（to match others）</li><li>K：Key（to be matched）</li><li>V：value（information to be extracted）</li></ul><p>关于self-Attention：</p><ol><li><p>计算Attention: 即使用每个query取对每个key做attention（表示的俩个向量之间有多接近，即通过内积：scaled inner product的方式来进行）$\alpha_{i,j}=q_i*k_j/\sqrt{d}$（d表示的是向量的维度，起到归一化的作用）</p></li><li><p>Softmax: $\alpha_{i,j}=exp(\alpha_{i,j}/\sum_k{\alpha_{i,k}})$表示的当前计算Attention进行softmax取值</p><p><img src="/2021/05/27/Transformer-In-Object-Detection/self-Attention.png" alt="Softmax" style="zoom:50%;"></p></li><li><p>获得该点于全局的关系：$b_i=\sum_k{\alpha_{i,k}*v_k}$</p><p><img src="/2021/05/27/Transformer-In-Object-Detection/self-Attention-Sum.png" alt="Long-Range Dependency" style="zoom:50%;"></p></li></ol><p>使用向量表示：$Attention(Q,K,V) = softmax(\frac{Q·K^T}{\sqrt{d}}V)$,其中$Q∈R^{n×d_k},K∈R^{m×d_k},V∈R^{m×d_v}$，即一个Attention层，将$n×d_k$的序列编码成了一个新的$n×d_v$的序列。</p><p>在此基础之上，文章中提出了Multi-Head Attention：</p><p><img src="/2021/05/27/Transformer-In-Object-Detection/Multi-Head Attention.png" alt="Multi-Head Attention" style="zoom:50%;"></p><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1,head_2,...,head_n) \\head_i=Attetnion(Q_i,K_i,V_i)</script><p>对于MultiHead的理解，可以是：对于一个token不同的子空间（一个head），它的注意力机制是不同的，比如拿词义来说，在head1中，表示的是指代关系，在head2中表示的是主谓宾关系。</p><p><font color="blue">那么在图像中，Multihead表示的对于一个图像切片的token，在不同的子空间对于其他切片token的关系不同，比如拿目标检测来说，在head1中表示，token之间是否存在有存在目标，在head2中表示token之间的关系</font>（理解待商榷）</p><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>关于Decoder相比于Encoder的区别在于，Decoder中有一个Masked Multi-Head Attention，并且其Multi-Head Attention使用的是cross attention机制，于Ecnoder中的self attention不同，Decoder的V，K来自Encoder的输出，而Q则来自Masked Multi-Head Attention的输出。</p><p><img src="/2021/05/27/Transformer-In-Object-Detection/Decoder.png" alt="image-20210608141458610" style="zoom:75%;"></p><h5 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi-Head Attention"></a>Masked Multi-Head Attention</h5><p>由于我们使用的是并行化的训练，因此需要设置mask来将后面的Ground Truth进行遮挡，即让当前的结果不会受到后面Ground Truth的影响。</p><p><img src="/2021/05/27/Transformer-In-Object-Detection/Masked.png" alt="Masked Multi-Head Attention"></p><h5 id="Multi-Head-Attention-1"><a href="#Multi-Head-Attention-1" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h5><p>将上一步Masked Multi-Head Attention作为Query输入到Multi-Head Attention中，并且其中的Key和Value均来自Encoder的输出。</p><p>我觉得这样设计的目的在于：因为Query作为一个引导信息，它需要找出Key和Value中比较重要的信息，因此Query使用结果作为输入，并且与Key和Value进行匹配，找出有用的信息。</p><h2 id="DETR"><a href="#DETR" class="headerlink" title="DETR"></a>DETR</h2><p>DETR不同于以往的目标检测任务，将</p><h2 id="Deformable-DETR"><a href="#Deformable-DETR" class="headerlink" title="Deformable DETR"></a>Deformable DETR</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Transformer-in-Object-Detection&quot;&gt;&lt;a href=&quot;#Transformer-in-Object-Detection&quot; class=&quot;headerlink&quot; title=&quot;Transformer in Object Detectio</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Object detection" scheme="https://ywj1.github.io/tags/Object-detection/"/>
    
    <category term="Transformer" scheme="https://ywj1.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Trident Network</title>
    <link href="https://ywj1.github.io/2020/10/26/Trident-Network/"/>
    <id>https://ywj1.github.io/2020/10/26/Trident-Network/</id>
    <published>2020-10-26T07:30:54.000Z</published>
    <updated>2020-10-26T08:14:36.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Scale-Aware-Trident-Networks-for-Object-Detection-用于物体检测的可感知规模的三叉戟网络"><a href="#Scale-Aware-Trident-Networks-for-Object-Detection-用于物体检测的可感知规模的三叉戟网络" class="headerlink" title="Scale-Aware Trident Networks for Object Detection(用于物体检测的可感知规模的三叉戟网络)"></a>Scale-Aware Trident Networks for Object Detection(用于物体检测的可感知规模的三叉戟网络)</h2><h3 id="初步阅读"><a href="#初步阅读" class="headerlink" title="初步阅读"></a>初步阅读</h3><p><strong>作者写作目的</strong></p><p>scale variation在目标检测中是一个十分重要的问题，因此作者提出了Trident block来进行多尺度特征检测。</p><p><strong>作者新颖的方法</strong></p><p>针对之前Feature Fusion中的一些问题，例如特征不连续等，提出了Trident Network，其实这个的关键在于Trident block，对于一个feature map使用三条支路（不同的带孔卷积）。</p><h3 id="进一步阅读"><a href="#进一步阅读" class="headerlink" title="进一步阅读"></a>进一步阅读</h3><h4 id="Trident-Networks"><a href="#Trident-Networks" class="headerlink" title="Trident Networks"></a>Trident Networks</h4><p>作者首先说明三个对于目标检测影响的重要因素：1.downsample rate 2. network depth 3. receptive field。对于第三个作者做了实验来说明该影响，对于大的scale物体，使用dilation大的带孔卷积效果会更好。</p><h5 id="Trident-block"><a href="#Trident-block" class="headerlink" title="Trident block"></a>Trident block</h5><p>于是作者在上述实验的基础之上，提出了Trident block，对于同一个模块，使用不同dilation的带孔卷积，从而达到不同scale的训练，并且针对FPN的feature inconsistency，作者的Trident block使用了uniform representational power。</p><p><img src="/2020/10/26/Trident-Network/network.jpg" alt="avatar"></p><p>并且作者提出了三个策略</p><ol><li>weigh sharing among branches，通过带孔卷积间的权重共享，可以减少参数个数，并且使用一个uniform transformation with the same representational power。</li><li>scale-aware training scheme，对于每个branch设置了不同的范围，之后对于不同的RoI使用不同的branch来进行训练。</li><li>使用middle branch来进行inference。加快了速度。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Scale-Aware-Trident-Networks-for-Object-Detection-用于物体检测的可感知规模的三叉戟网络&quot;&gt;&lt;a href=&quot;#Scale-Aware-Trident-Networks-for-Object-Detection-用于</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Object detection" scheme="https://ywj1.github.io/tags/Object-detection/"/>
    
    <category term="multi-scale" scheme="https://ywj1.github.io/tags/multi-scale/"/>
    
  </entry>
  
  <entry>
    <title>AFF</title>
    <link href="https://ywj1.github.io/2020/10/24/AFF/"/>
    <id>https://ywj1.github.io/2020/10/24/AFF/</id>
    <published>2020-10-24T05:56:50.000Z</published>
    <updated>2020-10-24T06:39:12.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Attention-Feature-Fusion-AFF-注意力机制的特征融合"><a href="#Attention-Feature-Fusion-AFF-注意力机制的特征融合" class="headerlink" title="Attention Feature Fusion(AFF:注意力机制的特征融合)"></a>Attention Feature Fusion(AFF:注意力机制的特征融合)</h2><h3 id="初步阅读-10-20"><a href="#初步阅读-10-20" class="headerlink" title="初步阅读(10%-20%)"></a>初步阅读(10%-20%)</h3><p><strong>作者写作目的</strong></p><p>To better fuse features of inconsistent semantics and scales. 因为之前的feature fusion有三个drawbacks:</p><ol><li>semantic inconsistency(语义不连续，模型通过feature fusion之后会造成语义不连续)</li><li>unsophisticated initial integration(简单的初步整合，对于俩个feature，仅仅通过element-wise summation)</li><li>仅仅对于global feature进行feature fusion，这会造成biased context aggregation scale</li></ol><p><strong>作者新颖之处</strong></p><p>作者基于SENet和SKNet提出了AFF模型，通过结合局部特征和全局特征的attention机制来进行feature fusion。</p><h3 id="进一步阅读"><a href="#进一步阅读" class="headerlink" title="进一步阅读"></a>进一步阅读</h3><h4 id="Attention-Feature-Fusion"><a href="#Attention-Feature-Fusion" class="headerlink" title="Attention Feature Fusion"></a>Attention Feature Fusion</h4><h5 id="MS-CAM-model"><a href="#MS-CAM-model" class="headerlink" title="MS_CAM model"></a>MS_CAM model</h5><p>下图是MS_CAM模型：</p><p><img src="/2020/10/24/AFF/MS_CAM.png" alt="avatar"></p><p>MS_CAM采用了bottleneck structure，左边是global feature，右边是local feature，其中左边的global feature与SENet不同之处在于，将SENet的Fully connected layer变为了Point-wise Conv layer，这样的目的是减小了计算量。其中+表示的element-wise summation，✖表示的element-wise multiplication。</p><h5 id="AFF-model"><a href="#AFF-model" class="headerlink" title="AFF model"></a>AFF model</h5><p><img src="/2020/10/24/AFF/AFF.png" alt="avatar"></p><p><img src="/2020/10/24/AFF/function-aff.png" alt="avatar"></p><p>其中Z表示的fused feature，我觉得该feature fusion想法类似于Cross Entropy。（理解起来还挺简单，但是想到是真的难！）对于其中的initial integration仅仅使用了简单的element-wise summation。因此作者在此基础之上又提出了iAFF，其实iAFF就是俩层AFF，对于X,Y在使用一次MS_CAM模型。如图：</p><p><img src="/2020/10/24/AFF/iAFF.png" alt="avatar"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Attention-Feature-Fusion-AFF-注意力机制的特征融合&quot;&gt;&lt;a href=&quot;#Attention-Feature-Fusion-AFF-注意力机制的特征融合&quot; class=&quot;headerlink&quot; title=&quot;Attention Feat</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Feature Fusion" scheme="https://ywj1.github.io/tags/Feature-Fusion/"/>
    
    <category term="Object detection" scheme="https://ywj1.github.io/tags/Object-detection/"/>
    
  </entry>
  
  <entry>
    <title>ConerNet</title>
    <link href="https://ywj1.github.io/2020/10/23/ConerNet/"/>
    <id>https://ywj1.github.io/2020/10/23/ConerNet/</id>
    <published>2020-10-23T01:43:03.000Z</published>
    <updated>2020-10-23T02:33:02.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CornerNet-Detecting-Objects-as-Paired-Keypoints-ConerNet-以成对的关键点检测目标"><a href="#CornerNet-Detecting-Objects-as-Paired-Keypoints-ConerNet-以成对的关键点检测目标" class="headerlink" title="CornerNet: Detecting Objects as Paired Keypoints(ConerNet:以成对的关键点检测目标)"></a>CornerNet: Detecting Objects as Paired Keypoints(ConerNet:以成对的关键点检测目标)</h2><h3 id="初步阅读-10-20"><a href="#初步阅读-10-20" class="headerlink" title="初步阅读(10%-20%)"></a>初步阅读(10%-20%)</h3><p><strong>作者写作目的？</strong></p><p>在之前的object detector中，由于anchor boxes的存在，会产生一些问题</p><ol><li>需要产生大量的anchor boxes</li><li>需要调整很多的参数，例如aspect ratios，scale等</li></ol><p>因此作者希望使用一个anchor-free detector来实现目标检测。</p><p><strong>新颖之处</strong></p><p>对于检测object，不像之前那样，需要检测center points，而是进行top-left and bottom-right的检测，并且设计了corner pooling来进行localize corner。作者借鉴了人体姿态估计的思想（embedding vector）。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;CornerNet-Detecting-Objects-as-Paired-Keypoints-ConerNet-以成对的关键点检测目标&quot;&gt;&lt;a href=&quot;#CornerNet-Detecting-Objects-as-Paired-Keypoints-Cone</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Object detection" scheme="https://ywj1.github.io/tags/Object-detection/"/>
    
    <category term="anchor-free" scheme="https://ywj1.github.io/tags/anchor-free/"/>
    
  </entry>
  
  <entry>
    <title>RetinaNet</title>
    <link href="https://ywj1.github.io/2020/10/18/RetinaNet/"/>
    <id>https://ywj1.github.io/2020/10/18/RetinaNet/</id>
    <published>2020-10-18T02:23:59.000Z</published>
    <updated>2020-10-18T06:01:36.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Focal-Loss-for-Dense-Object-Detection"><a href="#Focal-Loss-for-Dense-Object-Detection" class="headerlink" title="Focal Loss for Dense Object Detection"></a>Focal Loss for Dense Object Detection</h2><h3 id="初步阅读-10-20"><a href="#初步阅读-10-20" class="headerlink" title="初步阅读(10%-20%)"></a>初步阅读(10%-20%)</h3><h4 id="作者写作的目的"><a href="#作者写作的目的" class="headerlink" title="作者写作的目的"></a>作者写作的目的</h4><p>为了能够使one-stage的准确率能够提升到和two-stage差不多类似的精度，在不丧失速度的前提下。作者觉得这是由于one-stage的正负比例不均匀，导致训练之后的准确率不高。</p><h4 id="作者提出的新颖的方法"><a href="#作者提出的新颖的方法" class="headerlink" title="作者提出的新颖的方法"></a>作者提出的新颖的方法</h4><p>作者提出了一个新的损失函数(Focal Loss)，来进行平衡class imbalance。</p><h3 id="进一步阅读"><a href="#进一步阅读" class="headerlink" title="进一步阅读"></a>进一步阅读</h3><p>由于one-stage并没有像two-stage那样，提出region proposal，因此对于one-stage模型，会产生大量的negative samples，这就会造成class imbalance。class imbalance会造成俩个后果，引用原文：</p><ol><li>training is inefficient as most locations are easy negatives that contribute no useful learning signal.</li><li>en masse, the easy negatives can overwhelm training and lead to degenerate models.</li></ol><p>负样本数量过大，并且都是容易分类的样本，因此模型的优化并不是朝着我们希望的方向。因此本文针对类别不平衡，提出了一个新的loss function(Focal Loss function)。</p><h4 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h4><h5 id="Focal-Loss-Function"><a href="#Focal-Loss-Function" class="headerlink" title="Focal Loss Function"></a>Focal Loss Function</h5><p>在cross entropy和balanced cross entropy基础之上，作者提出了Focal Loss，it is defined as:</p><script type="math/tex; mode=display">\begin{equation}FL(p_t) =\begin{cases}-\alpha(1-p_t)^{\gamma}log(p_t),& y=1 \\(1-\alpha)(p_t)^{\gamma}log(1-p_t), & y = 0\\\end{cases}\end{equation}</script><p>因为balanced cross entropy能平衡正负样本，但是无法控制easy/hard examples，因此作者提出了Focal Loss，focus training on hard negatives。其中$\gamma$为调制系数(modulating factor)。目的是通过减少易分类样本的权重，从而使模型在训练时候更专注难分类的样本。$\alpha$可以控制正负样本权重。</p><p>对于Focal Loss有俩个重要的性质：</p><ol><li><p>当样本被分错之后，$p_t$很小，因此$(1-p_t)^{\gamma}$趋于1，相比于原来的loss没什么区别，当$p_t$趋于1时候(样本分类正确，且是易分类样本)，参数区域0，对于总的loss贡献很小。</p></li><li><p>$\gamma$增加时候，前面的参数也会增加。</p></li></ol><h5 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h5><p><img src="/2020/10/18/RetinaNet/RetinaNet.jpg" alt="avator"></p><p>使用ResNet+FPN为backbone，在每层的feature map中使用class subnet和box subnet。（具体如图所示）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Focal-Loss-for-Dense-Object-Detection&quot;&gt;&lt;a href=&quot;#Focal-Loss-for-Dense-Object-Detection&quot; class=&quot;headerlink&quot; title=&quot;Focal Loss for Den</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Object detection" scheme="https://ywj1.github.io/tags/Object-detection/"/>
    
    <category term="one-stage" scheme="https://ywj1.github.io/tags/one-stage/"/>
    
  </entry>
  
  <entry>
    <title>MobileNetv2</title>
    <link href="https://ywj1.github.io/2020/10/15/MobileNetv2/"/>
    <id>https://ywj1.github.io/2020/10/15/MobileNetv2/</id>
    <published>2020-10-15T12:13:32.000Z</published>
    <updated>2020-10-15T13:18:34.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks-MobileNetV2：残差和线性瓶颈"><a href="#MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks-MobileNetV2：残差和线性瓶颈" class="headerlink" title="MobileNetV2: Inverted Residuals and Linear Bottlenecks(MobileNetV2：残差和线性瓶颈)"></a>MobileNetV2: Inverted Residuals and Linear Bottlenecks(MobileNetV2：残差和线性瓶颈)</h2><h3 id="初步阅读-10-20"><a href="#初步阅读-10-20" class="headerlink" title="初步阅读(10%-20%)"></a>初步阅读(10%-20%)</h3><h4 id="作者写作的目的"><a href="#作者写作的目的" class="headerlink" title="作者写作的目的"></a>作者写作的目的</h4><p>保证模型的accuracy的同时降低模操作和内存消耗，使这个模型更能够部署在Mobile application中</p><h4 id="作者创新的方法"><a href="#作者创新的方法" class="headerlink" title="作者创新的方法"></a>作者创新的方法</h4><p>使用一个Inverted Residuals和Linear Bottlenecks来对原有的MobileNetv1进行改进。</p><h3 id="MobileNetV2"><a href="#MobileNetV2" class="headerlink" title="MobileNetV2"></a>MobileNetV2</h3><h4 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a>Network Structure</h4><p>MobileNetV2使用Inverted Residuals模块来进行构造网络结构。</p><h5 id="Inverted-Residuals"><a href="#Inverted-Residuals" class="headerlink" title="Inverted Residuals"></a>Inverted Residuals</h5><p>其实这个就是类似于Residuals network中的残差块，都是先用$1\times 1$卷积，使用$3\times 3$，再使用$1\times 1$进行操作，最后使用残差连接，其中不同的点在于残差网络中，先对原有的feature map进行降维，之后进行升维，Inverted Residuals对于feature map进行升维之后进行降维。</p><p><img src="/2020/10/15/MobileNetv2/difference-residual.jpg" alt="avatar"></p><h5 id="Body-Architecture"><a href="#Body-Architecture" class="headerlink" title="Body Architecture"></a>Body Architecture</h5><p><img src="/2020/10/15/MobileNetv2/network.png" alt="avatar"></p><p>本文提出了针对非线性激活函数提出了俩个猜想</p><ol><li>If the mainifold of interest remains non-zeros volume after ReLU transformation, it corresponds to a linear transformation.</li><li>ReLU is capable of preserving complete infromation about the input mainifold, but only if the input mainifold lies in a low-dimensional subspace of the input shape.</li></ol><p>第一个理论理解应该使简单因为$ReLU(x) = max(0,x)$，因此对于大于0的情况下，他就是线性的变化。对于猜想中的mainifold of interest理解成$w\times h\times c$的feature map上的一个点的c维特征，文章使用Linear Bottlenecks来进行说明网络的最后一层不使用ReLU非线性激活函数的原因。因为对于ReLU激活函数会造成信息的损失，但是随着输出维度的增加，信息的丢失会减小，因此使用线性层来减少信息的丢失。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks-MobileNetV2：残差和线性瓶颈&quot;&gt;&lt;a href=&quot;#MobileNetV2-Inverted-Residuals-and-Linear-Bottl</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="backbone" scheme="https://ywj1.github.io/tags/backbone/"/>
    
    <category term="classfication" scheme="https://ywj1.github.io/tags/classfication/"/>
    
  </entry>
  
  <entry>
    <title>MobileNetv1</title>
    <link href="https://ywj1.github.io/2020/10/15/MobileNetv1/"/>
    <id>https://ywj1.github.io/2020/10/15/MobileNetv1/</id>
    <published>2020-10-15T12:13:26.000Z</published>
    <updated>2020-10-15T12:49:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications-MobileNets：用于移动视觉应用的高效卷积神经网络"><a href="#MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications-MobileNets：用于移动视觉应用的高效卷积神经网络" class="headerlink" title="MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications(MobileNets：用于移动视觉应用的高效卷积神经网络)"></a>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications(MobileNets：用于移动视觉应用的高效卷积神经网络)</h2><h3 id="初步阅读-10-20"><a href="#初步阅读-10-20" class="headerlink" title="初步阅读(10%-20%)"></a>初步阅读(10%-20%)</h3><h4 id="作者写作的目的"><a href="#作者写作的目的" class="headerlink" title="作者写作的目的"></a>作者写作的目的</h4><p>为了能够将深度学习模型应用于移动应用中（例如手机等），因此需要降低模型的复杂度，从而降低对于运算时间和内存的消耗。</p><h4 id="作者的创新点"><a href="#作者的创新点" class="headerlink" title="作者的创新点"></a>作者的创新点</h4><p>使用Depthwise Separable Convolution(深度可分离卷积)和pointwise convolution(逐点卷积)来对传统的卷积操作进行切分，这样子就降低了模型的参数，从而就降低了运算时间和内存消耗。</p><h3 id="MobileNets"><a href="#MobileNets" class="headerlink" title="MobileNets"></a>MobileNets</h3><h4 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a>Network Structure</h4><p>网络的主要的单元是由Depthwise Separable Convolution和Pointwise Convolution来代替普通卷积来构成的。</p><p><img src="/2020/10/15/MobileNetv1/network.jpg" alt="avatar"></p><p>在每个深度可分离卷积和逐点卷积之后加上BN和ReLU层来进行操作。接下来分别说一说这俩个卷积。</p><h5 id="Depthwise-Separable-Convolution"><a href="#Depthwise-Separable-Convolution" class="headerlink" title="Depthwise Separable Convolution"></a>Depthwise Separable Convolution</h5><p>对于传统的卷积模型，要将$D_K\times D_k\times M$经过卷积成$D_K\times D_K\times N$,要使用(kernel_size = m)的卷积核$m\times m\times M\times N$</p><p>因此对于FLOPs，为$D_K\times D_K\times m\times m\times M\times N$，从理论上来说传统的卷积，是对于一个channel来说有$m\times m\times N$的size的卷积核来与feature map进行卷积操作。</p><p>对于Depthwise Separable Convolution，卷积核的一个channel对应于feature map的一个channel，因此只需要使用$m\times m \times M$ size的卷积核即可进行深度可分离卷积。</p><h5 id="Pointwise-Convolution"><a href="#Pointwise-Convolution" class="headerlink" title="Pointwise Convolution"></a>Pointwise Convolution</h5><p>对于深度可分离卷积操作过后的output我们是没办法改变它的channel的，因此需要逐点卷积来进行channel的改变，因此需要$1\times 1\times M \times N$size的卷积核。</p><p>因此对于上述操作之后，FLOPs为$D_K\times D_K \times m\times m\times M+D_K\times D_K\times M\times N$，因此相对于传统的卷积，FLOPs降低了$m^2$</p><h5 id="Body-Architecture"><a href="#Body-Architecture" class="headerlink" title="Body Architecture"></a>Body Architecture</h5><p>下图是它的模型的整体参数</p><p><img src="/2020/10/15/MobileNetv1/Body Architecture.jpg" alt="avatar"></p><p>对于模型我们引入俩个参数$\alpha$(宽度因子)和$\rho$(分辨率因子)来分别控制输入通道，输出通道的大小和输入图片的size。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications-MobileNets：用于移动视觉应用的高效卷积神经网络&quot;&gt;&lt;a href=&quot;#MobileNets</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="backbone" scheme="https://ywj1.github.io/tags/backbone/"/>
    
    <category term="classfication" scheme="https://ywj1.github.io/tags/classfication/"/>
    
  </entry>
  
  <entry>
    <title>EfficientDet</title>
    <link href="https://ywj1.github.io/2020/10/13/EfficientDet/"/>
    <id>https://ywj1.github.io/2020/10/13/EfficientDet/</id>
    <published>2020-10-13T07:35:23.000Z</published>
    <updated>2020-10-22T07:38:48.000Z</updated>
    
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="object detection" scheme="https://ywj1.github.io/tags/object-detection/"/>
    
    <category term="one-stage" scheme="https://ywj1.github.io/tags/one-stage/"/>
    
  </entry>
  
  <entry>
    <title>EfficientNet</title>
    <link href="https://ywj1.github.io/2020/10/13/EfficientNet/"/>
    <id>https://ywj1.github.io/2020/10/13/EfficientNet/</id>
    <published>2020-10-13T07:35:03.000Z</published>
    <updated>2020-10-22T07:38:28.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks-EfficientNet-对卷积神经网络缩放的重新思考"><a href="#EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks-EfficientNet-对卷积神经网络缩放的重新思考" class="headerlink" title="EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks(EfficientNet:对卷积神经网络缩放的重新思考)"></a>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks(EfficientNet:对卷积神经网络缩放的重新思考)</h2><h3 id="初步阅读-10-20"><a href="#初步阅读-10-20" class="headerlink" title="初步阅读(10%-20%)"></a>初步阅读(10%-20%)</h3><p><strong>作者写作目的?</strong></p><p>作者为了能够量化卷积神经网络的缩放（即深度，宽度和图片的解析率）对accuracy造成的影响，作者设计了compound scaling。同时扩展模型的不同维度来增加模型的效率，并保持accuracy增加。</p><p><strong>作者的新颖之处</strong></p><p>我觉得这篇文章的一个亮点，就是设计了compound scaling，将指标进行了量化，在增长accuracy的同时，提高了模型的efficiency。其中EfficientNet只是一个backbone，基于之前的模型设计出来的。</p><h3 id="进一步阅读"><a href="#进一步阅读" class="headerlink" title="进一步阅读"></a>进一步阅读</h3><p>从Net的名字可以看出，这是一个Efficient work，文章的主题思路，设计了一个compound scaling：</p><script type="math/tex; mode=display">depth:d = \alpha^\phi\\width:w=\beta^\phi\\resolution:r=\gamma^\phi\\s.t. \alpha\times\beta^2\times\gamma^2≈2</script><p>而这个compound scaling是从俩个observation中的得出的：</p><ul><li>Observation 1：对于网络的深度，宽度，分辨率中任何维度进行随访都可以提高精度，但是当模型足够大时候，会达到饱和。</li><li>Observation 2：为了追求更好的精度和效率，在缩放时平衡网络所有维度至关重要。</li></ul><p>为了确定constant参数，首先固定$\phi = 1$，进行一个小范围的搜索，最终确定的值为，$\alpha=1.2,\beta=1.1,\gamma=1.15$，之后的EfficientNetB1-B7都是基于该参数进行缩放的。</p><h4 id="EfficientNet"><a href="#EfficientNet" class="headerlink" title="EfficientNet"></a>EfficientNet</h4><p>EfficientNet基于mobile invered bottlenetc MBConv来进行设计，总体架构如下图所示：</p><p><img src="/2020/10/13/EfficientNet/Network.jpg" alt="avatar"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks-EfficientNet-对卷积神经网络缩放的重新思考&quot;&gt;&lt;a href=&quot;#EfficientNet-Rethinki</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="backbone" scheme="https://ywj1.github.io/tags/backbone/"/>
    
    <category term="classification" scheme="https://ywj1.github.io/tags/classification/"/>
    
  </entry>
  
  <entry>
    <title>SSD</title>
    <link href="https://ywj1.github.io/2020/10/09/SSD/"/>
    <id>https://ywj1.github.io/2020/10/09/SSD/</id>
    <published>2020-10-09T05:47:08.000Z</published>
    <updated>2020-10-14T12:42:12.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SSD-Single-Shot-MultiBox-Detector"><a href="#SSD-Single-Shot-MultiBox-Detector" class="headerlink" title="SSD: Single Shot MultiBox Detector"></a>SSD: Single Shot MultiBox Detector</h2><h3 id="初步阅读-10-20"><a href="#初步阅读-10-20" class="headerlink" title="初步阅读(10%-20%)"></a>初步阅读(10%-20%)</h3><p><strong>作者写作目的？</strong></p><p>加快检测速度，并提高准确率</p><p><strong>新颖的方法？</strong></p><p>SSD提取了不同尺度的特征图来进行预测。</p><h3 id="进一步阅读"><a href="#进一步阅读" class="headerlink" title="进一步阅读"></a>进一步阅读</h3><p>从名字可以读出SSD是一个多框预测算法，并且在准确度和速度上都比YOLO算法要好，SSD相对于YOLO算法有俩个很重要的改变：1. 提取了不同尺度的特征图来进行检测，使用大尺度来检测小物体，小尺度来进行检测大物体。2. 使用Default boxes(anchor boxes)来进行多框预测，并且不同于YOLO算法使用Fully connected，SSD使用$3\times 3$filter来进行结果预测。</p><h4 id="SSD模型"><a href="#SSD模型" class="headerlink" title="SSD模型"></a>SSD模型</h4><p>使用VGG模型(改造过)进行特征提取，在VGG提取之后，继续使用卷积核来继续进行特征图提取，并且对于不同的feature map使用$3\times 3$ filter来进行边界框预测和分类，最后使用NMS算法，删除多余的候选框，最后得到预测的边界框。(如下图)</p><p><img src="/2020/10/09/SSD/model.jpg" alt="avatar"></p><h5 id="SSD模型的改进之处"><a href="#SSD模型的改进之处" class="headerlink" title="SSD模型的改进之处"></a>SSD模型的改进之处</h5><p><strong>用于目标检测的多尺度特征图</strong></p><p>在改造过后的VGGNet使用卷积层继续提取feature map，使用后续的feature map来进行object detection。从而达到了多尺度的效果。(因为使用同一张图的不同feature map可以提高语义分割的质量，这类似于使用不同尺度的相同种类图片)</p><p><strong>使用卷积核来进行目标预测</strong></p><p>如上图所示，不同于YOLO算法在backbone之后使用fully connected layer，SSD使用convolutional layer来进行目标的预测。</p><p><strong>默认框和高宽比</strong></p><p>即使用了anchor boxes在每个特征图中。</p><h5 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h5><p><strong>匹配策略</strong></p><p>对于default boxes和ground truth，使用IoU大于0.5的当作正样本。</p><p><strong>目标函数</strong></p><script type="math/tex; mode=display">L(x,c,l,g) = \frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))</script><p>$L_{conf}$表示的分类损失函数，使用softmax loss函数来进行计算</p><script type="math/tex; mode=display">L_{conf}(x,c) = -\sum^N_{i\in Pos(T)}x_{ij}^plog(\hat{c}_i^p)-\sum_{i\in Neg(F)}log(\hat{c}_0^i)</script><script type="math/tex; mode=display">\hat{c}_i^p = \frac{exp(c_i^p)}{\sum_jexp(c_i^j)}​</script><p>$L_{loc}$表示的是回归损失函数，使用Smooth L1函数来进行计算，其中回归表示的边界框损失回归。(详见Faster R-CNN)</p><p><strong>为default boxes选不同的scales和aspect ratios</strong></p><p>scale of default boxes is defined as:</p><script type="math/tex; mode=display">s_k = s_{min} + \frac{s_{max} - s_{min}}{m-1}(k-1), k\in[1,m]</script><p>论文中设置$s_{min} = 0.2, s_{max} = 0.9，m = 6$，并且设置$aspect  ratios\in \{1,2,3,1/2,1/3\} $，对于ration为1的default boxes，继续设置scale为$\sqrt{s_ks_{k+1}}$，对于每个default boxes我们设置中心点为</p><script type="math/tex; mode=display">(\frac{i+0.5}{|f_k|},\frac{j+0.5}{|f_k|})</script><p>$|f_k|$表示的是第k个feature map的面积。</p><p>并且对于VGG中的feature map和最后一层feature map只设置4个default boxes，不设置3，1/3这俩个高宽比。</p><p><strong>设置训练样本</strong></p><p>按照default boxes产生的负样本会有很多，因此会产生正负样本比例失调，从而导致了欠拟合，因此我们将负样本按照IoU从大到小排序，并选取3：1的正负样本比例。</p><p><strong>VGGNet的改进</strong></p><p>将VGG16的全连接层fc6和fc7换成$3\times 3$的卷积层和$1\times 1$的卷积层，并将前一层的池化层使用Atrous Algorithm(带孔卷积\\扩展卷积)。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SSD-Single-Shot-MultiBox-Detector&quot;&gt;&lt;a href=&quot;#SSD-Single-Shot-MultiBox-Detector&quot; class=&quot;headerlink&quot; title=&quot;SSD: Single Shot MultiBox </summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Object detection" scheme="https://ywj1.github.io/tags/Object-detection/"/>
    
    <category term="one-stage" scheme="https://ywj1.github.io/tags/one-stage/"/>
    
  </entry>
  
  <entry>
    <title>AI在医学应用</title>
    <link href="https://ywj1.github.io/2020/10/08/AI%E5%9C%A8%E5%8C%BB%E5%AD%A6%E5%BA%94%E7%94%A8/"/>
    <id>https://ywj1.github.io/2020/10/08/AI%E5%9C%A8%E5%8C%BB%E5%AD%A6%E5%BA%94%E7%94%A8/</id>
    <published>2020-10-08T06:36:33.000Z</published>
    <updated>2020-10-08T06:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="注意的点"><a href="#注意的点" class="headerlink" title="注意的点"></a>注意的点</h4><p>two-stage相对于one-stage具有更高的精度，因此使用two-stage来进行检测</p><p>医学图像的正负样本不均衡，使用data augmentation来进行样本扩充。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;注意的点&quot;&gt;&lt;a href=&quot;#注意的点&quot; class=&quot;headerlink&quot; title=&quot;注意的点&quot;&gt;&lt;/a&gt;注意的点&lt;/h4&gt;&lt;p&gt;two-stage相对于one-stage具有更高的精度，因此使用two-stage来进行检测&lt;/p&gt;
&lt;p&gt;医学图像的正负</summary>
      
    
    
    
    
    <category term="Object detection" scheme="https://ywj1.github.io/tags/Object-detection/"/>
    
  </entry>
  
  <entry>
    <title>YOLO9000</title>
    <link href="https://ywj1.github.io/2020/10/07/YOLO9000/"/>
    <id>https://ywj1.github.io/2020/10/07/YOLO9000/</id>
    <published>2020-10-07T06:19:40.000Z</published>
    <updated>2020-10-07T06:19:42.000Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>论文常用单词与句子</title>
    <link href="https://ywj1.github.io/2020/09/29/%E8%AE%BA%E6%96%87%E5%B8%B8%E7%94%A8%E5%8D%95%E8%AF%8D%E4%B8%8E%E5%8F%A5%E5%AD%90/"/>
    <id>https://ywj1.github.io/2020/09/29/%E8%AE%BA%E6%96%87%E5%B8%B8%E7%94%A8%E5%8D%95%E8%AF%8D%E4%B8%8E%E5%8F%A5%E5%AD%90/</id>
    <published>2020-09-29T09:53:25.000Z</published>
    <updated>2020-09-29T10:11:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="论文常用单词与句子"><a href="#论文常用单词与句子" class="headerlink" title="论文常用单词与句子"></a>论文常用单词与句子</h2><h4 id="单词"><a href="#单词" class="headerlink" title="单词"></a>单词</h4><p>via    conj. 通过</p><p>low-resolution    n. 低分辨率</p><p>semantically strong feature    特征语义强</p><p>infeasible    adj.不可行的</p><p>spatial    adj.空间的</p><p>practical solution    实际的解决方案</p><p>frame……as……    视作，将这框为</p><p>refreshingly    adv.令人耳目一新地,及其</p><p>lag behind    落后</p><p>pretrain    预训练</p><p>resolution    分辨率(图片的尺寸)</p><p>invariant    不变的</p><p>design defect    设计缺陷</p><p>remarkable    卓越的</p><p>deep convolutional networks(ConvNets)</p><p>without bells and whistles    没有任何技巧</p><p>outperform    超过</p><p>tackle the problem    解决问题</p><p>methodology    方法</p><h4 id="句子"><a href="#句子" class="headerlink" title="句子"></a>句子</h4><p>The goal of this paper is to ……</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;论文常用单词与句子&quot;&gt;&lt;a href=&quot;#论文常用单词与句子&quot; class=&quot;headerlink&quot; title=&quot;论文常用单词与句子&quot;&gt;&lt;/a&gt;论文常用单词与句子&lt;/h2&gt;&lt;h4 id=&quot;单词&quot;&gt;&lt;a href=&quot;#单词&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="笔记" scheme="https://ywj1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="https://ywj1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>AugFPN</title>
    <link href="https://ywj1.github.io/2020/09/27/AugFPN/"/>
    <id>https://ywj1.github.io/2020/09/27/AugFPN/</id>
    <published>2020-09-27T05:16:09.000Z</published>
    <updated>2020-10-24T05:58:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AugFPN-Improving-Multi-scale-Feature-Learning-for-Object-Detection-改进用于目标检测的多尺度特征学习"><a href="#AugFPN-Improving-Multi-scale-Feature-Learning-for-Object-Detection-改进用于目标检测的多尺度特征学习" class="headerlink" title="AugFPN: Improving Multi-scale Feature Learning for Object Detection(改进用于目标检测的多尺度特征学习)"></a>AugFPN: Improving Multi-scale Feature Learning for Object Detection(改进用于目标检测的多尺度特征学习)</h2><h3 id="初步阅读（10-20-）"><a href="#初步阅读（10-20-）" class="headerlink" title="初步阅读（10%-20%）"></a>初步阅读（10%-20%）</h3><p><strong>作者的写作目的？</strong></p><p>为了提高FPN网络的性能，在FPN网络基础之上，对FPN网络进行改进。首先分析FPN网络各个模块的问题（<font color="red">3个问题</font>)(待解决)，之后提出了AugFPN，AugFPN分为三个模块（待解决）</p><p><strong>作者用的方法？</strong></p><p>提出AugFPN网络，AugFPN网络分为三个类别：Consistant Supervision，Residual Feature Augmentation，Soft ROI Selection</p><p><strong>FPN存在的三个问题</strong></p><ol><li>before feature fusion: 对于原图像使用$1\times1$卷积减少通道时候，会减弱图像的多尺度特征表达（水平方向）</li><li>top-down feature fusion: 在特征融合过程中，顶层由于减少了通道，会造成最高层特征图的信息减少（垂直方向）</li><li>after feature fusion: <font color="red">FPN中每个候选区域(ROI)的特征都是根据proposal的尺度来决定相应的特征图从哪一层选择，然而\那些忽略的层也是包含着丰富信息的，它们对最终的分类和回归结果也有着影响。直接忽略了其它层的特征势必会影响到最终的检测结果。</font></li></ol><p><strong>AugFPN三个策略解决问题</strong></p><ol><li>Consistent Supervision</li><li>Residual Feature Augmentation</li><li>Soft RoI Selection</li></ol><hr><h3 id="进一步阅读-20-60"><a href="#进一步阅读-20-60" class="headerlink" title="进一步阅读(20%-60%)"></a>进一步阅读(20%-60%)</h3><h4 id="design-defects"><a href="#design-defects" class="headerlink" title="design defects"></a>design defects</h4><p>文章根据FPN的三个不同阶段的三个缺陷提出了不同的解决方法，最后将这三个方法整合在一起，形成了AugFPN。</p><p>这三个不同阶段分别是特征融合(feature fusion)前，特征融合中，特征融合后</p><p>特征融合前：由于在特征融合之前，对于backbone生成的不同feature map要使用$1\times 1$卷积，将会降低多尺度特征表达(??)，因为没有考虑semantic gap。(？？)使用存在语义差异的特征进行融合，可能会导致次优解。</p><p>特征融合中：对于top-down pathway的feature map和backbone的feature map进行element-wise相加，可能会损失空间解析率和空间细节，因为可能有多个物体存在一个照片中。</p><p>特征融合后：每个目标的proposal都是基于从一个特征层次上汇聚的特征网格进行细化的，这些特征网格是根据proposals的尺度启发式地选择的。但是，来自其他级别的被忽略的特性可能对对象分类或回归有益。考虑到这个问题，PANet集合了所有金字塔层的RoIs特征，并将它们以max操作方式融合，然后将它们与独立的全连接层融合。然而，max融合会忽略响应较小的特性，而这些特性可能也很有帮助，并且仍然不能充分利用其他级别的特性。同时，额外的全连通层显著增加了模型参数（不太懂，只是翻译）</p><h4 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h4><p><img src="/2020/09/27/AugFPN/architecture.jpg" alt="avatar"></p><h5 id="Consistent-Supervision"><a href="#Consistent-Supervision" class="headerlink" title="Consistent Supervision"></a>Consistent Supervision</h5><p>对融合后的特征图M2~M5进行监督，具体方法使对RPN网络得到的每一个候选区域分别映射到M2~M5上的到相应的feature map，对feature map进行分类和回归，这样得到一个损失函数，将这个损失函数和网络本身的损失进行一个加权求和。</p><h5 id="Residual-Feature-Augmentation"><a href="#Residual-Feature-Augmentation" class="headerlink" title="Residual Feature Augmentation"></a>Residual Feature Augmentation</h5><p>在融合之前使用$1\times 1$的卷积进行特征降维，会造成信息损失，因为C5的信息使没有损失的，因此将C5的特征融合到M5中，减少了信息损失。</p><h5 id="Soft-RoI-Selection"><a href="#Soft-RoI-Selection" class="headerlink" title="Soft RoI Selection"></a>Soft RoI Selection</h5>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;AugFPN-Improving-Multi-scale-Feature-Learning-for-Object-Detection-改进用于目标检测的多尺度特征学习&quot;&gt;&lt;a href=&quot;#AugFPN-Improving-Multi-scale-Feature-</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Feature Fusion" scheme="https://ywj1.github.io/tags/Feature-Fusion/"/>
    
    <category term="Object detection" scheme="https://ywj1.github.io/tags/Object-detection/"/>
    
  </entry>
  
  <entry>
    <title>FPN</title>
    <link href="https://ywj1.github.io/2020/09/26/FPN/"/>
    <id>https://ywj1.github.io/2020/09/26/FPN/</id>
    <published>2020-09-26T05:55:23.000Z</published>
    <updated>2020-10-24T05:58:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Feature-Pyramid-Networks-for-Object-Detection-用于目标检测的特征金字塔网络"><a href="#Feature-Pyramid-Networks-for-Object-Detection-用于目标检测的特征金字塔网络" class="headerlink" title="Feature Pyramid Networks for Object Detection(用于目标检测的特征金字塔网络)"></a>Feature Pyramid Networks for Object Detection(用于目标检测的特征金字塔网络)</h2><h3 id="初步阅读-10-40"><a href="#初步阅读-10-40" class="headerlink" title="初步阅读(10%-40%)"></a>初步阅读(10%-40%)</h3><p><strong>作者的写作目的是什么？</strong></p><p>为了解决目标检测算法在处理多尺度变化问题时候的不足，因为Faster R-CNN, Fast R-CNN都是基于单个高层特征（即通过CNN提取后的特征图）来进行目标检测的，这种方法对于小物体不是特别友好（因为提取之后的特征图语义特征比较强，分辨率比较低）因此对于小物体定位并不是很好，因此设计出来特征金字塔来进行特征的提取，使不同尺度的特征具有较强的语义信息。</p><p><strong>作者提出的方法？</strong></p><p>作者提出了FPN(Feature Pyramid Network)来构建特征金字塔，由于现有的特征金字塔网络回答来极大的计算量和内存需求。</p><p>FPN网络有俩个层次构成，一个是自下而上的过程，另一个是自顶向下(top-down architecture)和侧向连接(lateral connections)</p><p><img src="/2020/09/26/FPN/Network.jpg" alt="avatar"></p><hr><h3 id="进一步阅读-40-80"><a href="#进一步阅读-40-80" class="headerlink" title="进一步阅读(40%-80%)"></a>进一步阅读(40%-80%)</h3><p>特征金字塔是CV中常用的技巧，主要目的是能够让网络识别不同尺度中的目标，使网络具有尺度不变性(scale-invariant)。但是随着DL的应用，feature pyramid使用的比较少。</p><ul><li>首先因为CNN就具有较强的robust再尺度改变的情况下</li><li>在DL中使用特征金字塔，会产生很大的内存开销和更长的运算时间，在实践中基本只会在测试中使用，这就会造成train/test不连续。</li></ul><h4 id="CNN提取图像特征的性质"><a href="#CNN提取图像特征的性质" class="headerlink" title="CNN提取图像特征的性质"></a>CNN提取图像特征的性质</h4><p>我们知道CNN使用Conv/Pooling来操作图像，不同深度的layer会产生不同的feature map和spatial dimension，深层的feature map具有更高的语义特征（semantically strong features）和低分辨率（low-resoltion），底层的feature map则相反。因此该文章的目的使通过结合不同的feature map形成一个在不同尺度下有强语义特征的图像。</p><p>对比不同的Feature Pyramid:</p><p><img src="/2020/09/26/FPN/Pyramid-feature.png" alt="avatar"></p><p>(a): 将图像做成金字塔，分别使用一个NN来进行预测，计算量很大(HOG)</p><p>(b): 只在最后一层的feature map来进行预测，只在single scale(faster R-CNN等)</p><p>(c): 将各个layer的输出feature map堪称feature pyramid来进行(SSD)</p><p>(d): 本文的做法，对于不同层级的feature map进行merge，使每个level的语义信息比较强。</p><h4 id="FPN具体实现"><a href="#FPN具体实现" class="headerlink" title="FPN具体实现"></a>FPN具体实现</h4><p><strong>Bottom-up pathway</strong>: 就是一个前向传播，相当于一个backbone，进行特征的提取。</p><p><strong>Top-down pathway and lateral connection</strong>：将上层的feature map进行upsamling（高层的feature map有更强的语义特征，通过upsampling变成有更高分辨率），并且与bottom-up获</p><p>得feature map（使用$1\times 1$卷积改变通道）进行lateral connection进行增强。如下图所示：</p><p><img src="/2020/09/26/FPN/lateral-connection.png" alt="avatar"></p><p>高层的feature map经过upsampling和横向的bottom-up的feature map进行element-wise的相加。</p><p>并且为了减少上采样的混叠效果(reduce the aliasing effect of upsampling)，对于每个merged map使用一个$3\times 3$卷积来进行处理。</p><p>下面使tensorflow实现：</p><pre class="line-numbers language-lang-python"><code class="language-lang-python">import  tensorflow as tffrom    tensorflow.keras import layersclass FPN(tf.keras.Model):    def __init__(self, out_channels=256, **kwargs):        '''        Feature Pyramid Networks        Attributes        ---            out_channels: int. the channels of pyramid feature maps.        '''        super(FPN, self).__init__(**kwargs)        self.out_channels = out_channels        #lateral connection        self.fpn_c2p2 = layers.Conv2D(out_channels, (1, 1),                                       kernel_initializer='he_normal', name='fpn_c2p2')        self.fpn_c3p3 = layers.Conv2D(out_channels, (1, 1),                                       kernel_initializer='he_normal', name='fpn_c3p3')        self.fpn_c4p4 = layers.Conv2D(out_channels, (1, 1),                                       kernel_initializer='he_normal', name='fpn_c4p4')        self.fpn_c5p5 = layers.Conv2D(out_channels, (1, 1),                                       kernel_initializer='he_normal', name='fpn_c5p5')        #top-down pathway        self.fpn_p3upsampled = layers.UpSampling2D(size=(2, 2), name='fpn_p3upsampled')        self.fpn_p4upsampled = layers.UpSampling2D(size=(2, 2), name='fpn_p4upsampled')        self.fpn_p5upsampled = layers.UpSampling2D(size=(2, 2), name='fpn_p5upsampled')        #减少upsampling的混叠效果        self.fpn_p2 = layers.Conv2D(out_channels, (3, 3), padding='SAME',                                     kernel_initializer='he_normal', name='fpn_p2')        self.fpn_p3 = layers.Conv2D(out_channels, (3, 3), padding='SAME',                                     kernel_initializer='he_normal', name='fpn_p3')        self.fpn_p4 = layers.Conv2D(out_channels, (3, 3), padding='SAME',                                     kernel_initializer='he_normal', name='fpn_p4')        self.fpn_p5 = layers.Conv2D(out_channels, (3, 3), padding='SAME',                                     kernel_initializer='he_normal', name='fpn_p5')        self.fpn_p6 = layers.MaxPooling2D(pool_size=(1, 1), strides=2, name='fpn_p6')    def call(self, inputs, training=True):        C2, C3, C4, C5 = inputs        P5 = self.fpn_c5p5(C5)        P4 = self.fpn_c4p4(C4) + self.fpn_p5upsampled(P5)        P3 = self.fpn_c3p3(C3) + self.fpn_p4upsampled(P4)        P2 = self.fpn_c2p2(C2) + self.fpn_p3upsampled(P3)        # Attach 3x3 conv to all P layers to get the final feature maps.        P2 = self.fpn_p2(P2)        P3 = self.fpn_p3(P3)        P4 = self.fpn_p4(P4)        P5 = self.fpn_p5(P5)        # subsampling from P5 with stride of 2.        P6 = self.fpn_p6(P5)        return [P2, P3, P4, P5, P6]    def compute_output_shape(self, input_shape):        C2_shape, C3_shape, C4_shape, C5_shape = input_shape        C2_shape, C3_shape, C4_shape, C5_shape = \            C2_shape.as_list(), C3_shape.as_list(), C4_shape.as_list(), C5_shape.as_list()        C6_shape = [C5_shape[0], (C5_shape[1] + 1) // 2, (C5_shape[2] + 1) // 2, self.out_channels]        C2_shape[-1] = self.out_channels        C3_shape[-1] = self.out_channels        C4_shape[-1] = self.out_channels        C5_shape[-1] = self.out_channels        return [tf.TensorShape(C2_shape),                tf.TensorShape(C3_shape),                tf.TensorShape(C4_shape),                tf.TensorShape(C5_shape),                tf.TensorShape(C6_shape)]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Feature-Pyramid-Networks-for-Object-Detection-用于目标检测的特征金字塔网络&quot;&gt;&lt;a href=&quot;#Feature-Pyramid-Networks-for-Object-Detection-用于目标检测的特征金字塔网络</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Feature Fusion" scheme="https://ywj1.github.io/tags/Feature-Fusion/"/>
    
    <category term="Object detection" scheme="https://ywj1.github.io/tags/Object-detection/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv1</title>
    <link href="https://ywj1.github.io/2020/09/22/YOLOv1/"/>
    <id>https://ywj1.github.io/2020/09/22/YOLOv1/</id>
    <published>2020-09-22T05:57:59.000Z</published>
    <updated>2020-10-07T05:59:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection-统一，实时的目标检测"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection-统一，实时的目标检测" class="headerlink" title="You Only Look Once: Unified, Real-Time Object Detection(统一，实时的目标检测)"></a>You Only Look Once: Unified, Real-Time Object Detection(统一，实时的目标检测)</h2><h4 id="初步阅读-10-20"><a href="#初步阅读-10-20" class="headerlink" title="初步阅读(10%-20%)"></a>初步阅读(10%-20%)</h4><p><strong>作者写作目的？</strong></p><p>因为two-stage的目标检测耗时长，并且难训练（因为每个部分是独立的），无法做到实时的检测，因此作者提出了one-stage的算法Yolo。</p><p><strong>作者提出新的方法？</strong></p><p>作者将目标检测视为回归问题，使用一个整合的网络，来进行目标检测的训练，最后输出位置，类别等信息。</p><h4 id="进一步阅读"><a href="#进一步阅读" class="headerlink" title="进一步阅读"></a>进一步阅读</h4><p><strong>YOLO算法的优势</strong></p><ol><li>faster</li><li>reason globally about the image：在预测图片时候，可以分析全局的图像，不像two-stage那样，只分析region proposal</li><li>learn the generalizable representations of object：学习概述的特征，可以用于艺术的识别</li></ol><p><strong>YOLO模型的框架</strong></p><p><img src="/2020/09/22/YOLOv1/YOLOv1.png" alt="The architecture"></p><p>输入resize: $448\times 448\times 3$</p><p>输出: $7\times 7\times 30$</p><p>其中输出为$7\times 7$表示的是将图像分为$7\times 7$的网格，最后30分为20和10，10为俩个bounding box(坐标和confidence: 表示是否是物体)，20为一个20个类别。</p><p>bounding box: 四个坐标(x,y,w,h), confidence表示该边界框是否包含目标，$confidence = Pr(object)*IOU_{pred}^{truth}$</p><p>激活函数使用Leaky relu函数来进行</p><p><img src="/2020/09/22/YOLOv1/loss_function.png" alt="loss function"></p><p>其中$\lambda_{coord} = 5, \lambda_{noobj} = 0.5$</p><p><strong>YOLO的缺点</strong></p><p>对于小物体效果不好，并且一个网格中只能预测了俩个框，并且只属于一类。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;You-Only-Look-Once-Unified-Real-Time-Object-Detection-统一，实时的目标检测&quot;&gt;&lt;a href=&quot;#You-Only-Look-Once-Unified-Real-Time-Object-Detection-统一</summary>
      
    
    
    
    <category term="论文笔记" scheme="https://ywj1.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Object detection" scheme="https://ywj1.github.io/tags/Object-detection/"/>
    
    <category term="one-stage" scheme="https://ywj1.github.io/tags/one-stage/"/>
    
  </entry>
  
  <entry>
    <title>如何写学术论文</title>
    <link href="https://ywj1.github.io/2020/09/21/%E5%A6%82%E4%BD%95%E5%86%99%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87/"/>
    <id>https://ywj1.github.io/2020/09/21/%E5%A6%82%E4%BD%95%E5%86%99%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87/</id>
    <published>2020-09-21T13:57:52.000Z</published>
    <updated>2020-09-25T12:04:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何书写学术论文（学术论文-70-思考-计划-30-写作-修改）"><a href="#如何书写学术论文（学术论文-70-思考-计划-30-写作-修改）" class="headerlink" title="如何书写学术论文（学术论文 = 70%思考/计划+30%写作/修改）"></a>如何书写学术论文（学术论文 = 70%思考/计划+30%写作/修改）</h2><h3 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h3><ol><li>首先有个好的idea（标题很重要）<ul><li>寻找idea的几种思路（全新的发现，新的方法，新理论或者新模型，新应用，改进原有的方法获得更好的性能）后俩者比较容易</li><li>多读文献多思考（必须多看，才有想法，才会对这一块了解）</li></ul></li><li>想好文章的核心观点</li><li>构思论文的大框架</li><li>提前整理好数据</li><li>论文写作和修改<ul><li>多英文文献时候（作者是老外），记录好的句型和好的词汇（修饰词和专业术语）</li><li>Grammy插件</li></ul></li><li>投稿</li></ol><h3 id="如何思考"><a href="#如何思考" class="headerlink" title="如何思考"></a>如何思考</h3><p><strong>画出你的研究（Picturing your research）</strong>：</p><ol><li><p>what？ </p><ul><li>main research question文章主要讲的是什么</li><li>Disciplines 研究问题涵盖的所有领域或者学科</li></ul></li><li><p>why？</p><ul><li>Reasons 为什么做这个研究（创新点是什么）Most important</li></ul></li><li>how？<ul><li>Methods 如何去解决这个问题</li></ul></li></ol><h3 id="书写方法"><a href="#书写方法" class="headerlink" title="书写方法"></a>书写方法</h3><h5 id="引言部分"><a href="#引言部分" class="headerlink" title="引言部分"></a>引言部分</h5><p>找十篇相关文献，研究写作思路，之后构思自己的研究思路，在进行模仿写作。</p><ul><li>第一段介绍课题背景或者起源</li><li>第二段介绍前辈的研究动态，引用大量文献（这个一定要自我整理）</li><li>总结自己的研究，主要解决的是什么问题，在哪些方面做出贡献等</li></ul><h3 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h3><p>标题：有一个简明扼要并且具有吸引力的标题</p><hr><h2 id="如何写好论文和评审（笔记）"><a href="#如何写好论文和评审（笔记）" class="headerlink" title="如何写好论文和评审（笔记）"></a>如何写好论文和评审（笔记）</h2><h3 id="如何写好Paper"><a href="#如何写好Paper" class="headerlink" title="如何写好Paper"></a>如何写好Paper</h3><ol><li>首先确定自己的主题</li><li>简单介绍其他方法，为什么不够好</li><li>介绍你的方法，与其他方法比较，优势在哪里</li><li>最后，介绍相关工作</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;如何书写学术论文（学术论文-70-思考-计划-30-写作-修改）&quot;&gt;&lt;a href=&quot;#如何书写学术论文（学术论文-70-思考-计划-30-写作-修改）&quot; class=&quot;headerlink&quot; title=&quot;如何书写学术论文（学术论文 = 70%思考/计划+30%</summary>
      
    
    
    
    <category term="笔记" scheme="https://ywj1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="论文书写" scheme="https://ywj1.github.io/tags/%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99/"/>
    
  </entry>
  
  <entry>
    <title>hexo搭建博客</title>
    <link href="https://ywj1.github.io/2020/09/21/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>https://ywj1.github.io/2020/09/21/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-09-21T02:31:00.000Z</published>
    <updated>2020-09-28T02:56:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>当hexo g -d发生以下错误时候</p><p><img src="/2020/09/21/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/error.png" alt="avatar"></p><p>使用以下命令可以解决</p><pre><code>git config --global user.email &quot;you@example.com&quot; git config --global user.name &quot;Your Name&quot;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;当hexo g -d发生以下错误时候&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2020/09/21/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/error.png&quot; alt=&quot;avatar&quot;&gt;&lt;/p&gt;
&lt;p&gt;使用以下命令可以解决&lt;/p&gt;
&lt;</summary>
      
    
    
    
    <category term="笔记" scheme="https://ywj1.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="https://ywj1.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
